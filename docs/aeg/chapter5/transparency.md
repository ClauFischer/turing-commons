# Transparency and Explainability

## Introduction to transparency and explainability

### Defining transparent AI

Transparency as a principle of AI ethics differs a bit in meaning from the everyday use of the term. The common dictionary understanding of transparency defines it as either (1) the quality an object has when one can see clearly through it or (2) the quality of a situation or process that can be clearly justified and explained because it is open to inspection and free from secrets. 
 
Transparency as a principle of AI ethics encompasses both of these meanings:
 
On the one hand, transparent AI involves the interpretability of a given AI system, i.e. the ability to know how and why a model performed the way it did in a specific context and therefore to understand the rationale behind its decision or behaviour. This sort of transparency is often referred to by way of the metaphor of ‘opening the black box’ of AI. It involves content clarification and intelligibility or explicability.
 
On the other hand, transparent AI involves the justifiability of both of the processes that go into its design and implementation and of its outcome. It therefore involves the soundness of the justification of its use. In this more normative meaning, transparent AI is practically justifiable in an unrestricted way if one can demonstrate that both the design and implementation processes that have gone into the particular decision or behaviour of a system and the decision or behaviour itself are sustainable, safe, fair, and driven by responsibly managed data. 

### Process-based and outcome-based explanations

The two-pronged definition of transparency as a principle of AI ethics asks that you think about transparent AI in terms of the process behind it (the design and implementation practices that lead to an algorithmically supported outcome) and in terms of its product (the content and justification of that outcome), and that you provide explanations to impacted stakeholders that: 
Demonstrate how you and all others involved in the development of your system acted responsibly when choosing the processes behind its design and deployment; and
make the reasoning behind the outcome of that decision clear.

Process-based explanations of AI systems are about demonstrating that you have followed good governance processes and best practices throughout your design and use. This entails demonstrating that considerations of sustainability, safety, fairness, and responsible data management were operative end-to-end in the project lifecycle. 

For example, if you are trying to explain the fairness and safety of a particular AI-assisted decision, one component of your explanation will involve establishing that you have taken adequate measures across the system’s production and deployment to ensure that its outcome is fair and safe.
Considerations for Children-Centred AI: When considering AI systems that are developed using children’s data, in order to explain that the system was fair and safe, one must first ensure that the Age Appropriate Design Code was adhered to. However, a process-based explanation in this setting expands well past legal compliance. How have adequate measures of reporting and oversight been put in place to ensure that children are not harmed in the process of the design, development, and deployment of AI technologies that use their data?
 
Outcome-based explanations of AI systems are about clarifying the results of a specific decision. They involve explaining the reasoning behind a particular algorithmically generated outcome in plain, easily understandable, and everyday language that is socially meaningful to impacted stakeholders (understandable in terms of the contextual factors and relationships that it implicates). 

- If there is meaningful human involvement in the decision-making process, you also have to make clear to the affected individual how and why a human judgement that is assisted by an AI output was reached.
In addition, you may also need to confirm that the actual outcome of an AI decision meets the criteria that you established in your design process to ensure that the AI system is being used in a fair, safe, and ethical way. In offering an explanation to affected stakeholders, you should be able to demonstrate that a specific decision or behaviour of your system is sustainable, safe, fair, and driven by data that has been responsibly managed. 

### Guideline for building appropriately explainable AI systems

**Guideline 1:**
Look first to context, potential impact, and domain-specific need when determining the interpretability requirements of your project
- Type of application
- Domain specificity
- Existing tech

**Guideline 2:**
Draw on standard interpretable techniques when possible
- Find the right fit between 1) domain-specific risks and needs, 2) available data resources and domain knowledge, and 3) task appropriate machine learning techniques

**Guideline 3:**
When considering the use of ‘black box’ AI systems, you should:
- Thoroughly weigh up impacts and risks
- Consider options availability for supplemental interpretability tools
- Formulate interpretability action plan

**Guideline 4:**
Think about interpretability in terms of the capacities of human understanding

### Four principles of AI explainability

The following principles provide a broad steer on what to think about when explaining AI-assisted decisions to individuals.

!!! abstract

    1. Be transparent
    2. Be accountable
    3. Consider context
    4. Reflect on impacts
    
### Be transparent
**What is this principle about?**
 
The principle of being transparent is an extension of the transparency aspect of principle a) in the GDPR (lawfulness, fairness and transparency). In data protection terms, transparency means being open and honest about who you are, and how and why you use personal data. Being transparent about AI-assisted decisions builds on these requirements. It is about making your use of AI for decision-making obvious and appropriately explaining the decisions you make to individuals in a meaningful way.
 
**What are the key aspects of being transparent?**
 
Raise awareness
    - Be open and candid about:
          - your use of AI-enabled decisions;
          - when you use them; and
          - why you choose to do this.

    - Proactively make people aware of a specific AI-enabled decision concerning them, in advance of making the decision.
    - Meaningfully explain decisions:
    - Don’t just give any explanation to people about AI-enabled decisions - give them:
          - a truthful and meaningful explanation;
          - written or presented appropriately; and
          - delivered at the right time.

### Be accountable
**What is this principle about?**

The principle of being accountable is derived from the accountability principle in the GDPR. In data protection terms, accountability means taking responsibility for complying with the other data protection principles and being able to demonstrate that compliance. It also means implementing appropriate technical and organisational measures, and data protection by design and default. Being accountable for explaining AI-assisted decisions concentrates these dual requirements on the processes and actions you carry out when designing (or procuring/ outsourcing) and deploying AI models.
 
It is about ensuring appropriate oversight of your AI decision systems, and being answerable to others in your organisation, to external bodies such as regulators, and to the individuals you make AI-assisted decisions about.
 
**What are the key aspects of being accountable?**
Assign responsibility:
    - Identify those within your organisation who manage and oversee the ‘explainability’ requirements of an AI decision system and assign ultimate responsibility for this.
    - Ensure you have a designated and capable human point of contact for individuals to query or contest a decision.

Justify and evidence:
    - Actively consider and make justified choices about how to design and deploy AI models that are appropriately explainable to individuals.
    - Take steps to prove that you made these considerations, and that they are present in the design and deployment of the models themselves.
    -  Show that you provided explanations to individuals.

### Consider context
**What is this principle about?**
There is no one-size-fits-all approach to explaining AI-assisted decisions. The principle of considering context underlines this. It is about paying attention to several different, but interrelated, elements that can have an effect on explaining AI-assisted decisions and managing the overall process. This is not a one-off consideration. It’s something you should think about at all stages of the process, from concept to deployment and presentation of the explanation to the decision recipient.
 
**What are the key aspects of considering context?**
 
Choose appropriate models and explanation
 
When planning on using AI to help make decisions about people, you should consider:
    - the setting in which you will do this;
    - the potential impact of the decisions you make
    - what an individual should know about a decision, so you can choose an appropriately explainable AI model; and
    - prioritising delivery of the relevant explanation types.
 
Tailor governance and explanation
 
Your governance of the ‘explainability’ of AI models should be:
    - robust and reflective of best practice; and
    - tailored to your organisation and the particular circumstances and needs of each decision recipient.

### Reflect on impacts
**What is this principle about?**
In making decisions and performing tasks that have previously required the thinking and reasoning of responsible humans, AI systems are increasingly serving as trustees of human decision-making. However, individuals cannot hold these systems directly accountable for the consequences of their outcomes and behaviours.
 
The value of reflecting on the impacts of your AI system helps you explain to individuals affected by its decisions that the use of AI will not harm or impair their wellbeing. This means asking and answering questions about the ethical purposes and objectives of your AI project at the initial stages.
 
You should then revisit and reflect on the impacts identified in the initial stages of the AI project throughout the development and implementation stages. If any new impacts are identified, you should document them, alongside any mitigating factors you implement where relevant. This will help you explain to decision recipients what impacts you have identified and how you have reduced any potentially harmful effects as far as possible.

**What are the key aspects of reflecting on impacts?**
**Individual wellbeing**

Think about how to build and implement your AI system in a way that:
    - fosters the physical, emotional and mental integrity of affected individuals;
    - ensures their abilities to make free and informed decisions about their own lives;
    - safeguards their autonomy and their power to express themselves;
    - supports their abilities to flourish, to fully develop themselves, and to pursue their interests according to their own freely determined life plans;
    - preserves their ability to maintain a private life independent from the transformative effects of technology; and
    - secures their capacities to make well-considered, positive and independent contributions to their social groups and to the shared life of the community, more generally.
    
**Well-being of wider society**
Think about how to build and implement your AI system in a way that:
    - safeguards meaningful human connection and social cohesion;
    - prioritises diversity, participation and inclusion;
    - encourages all voices to be heard and all opinions to be weighed seriously and sincerely;
    - treats all individuals equally and protects social equity;
    - uses AI technologies as an essential support for the protection of fair and equal treatment under the law;
    - utilises innovation to empower and to advance the interests and well-being of as many individuals as possible; and
    - anticipates the wider impacts of the AI technologies you are developing by thinking about their ramifications for others around the globe, for the biosphere as a whole and for future generations.

## Six main types of explanations

!!!! abstract

    1. Rationale explanation
    2. Responsibility explanation
    3. Data explanation
    4. Fairness explanation
    5. Safety and performance explanation
    6. Impact explanation

