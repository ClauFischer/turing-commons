# AI Harms

## 1. Loss of autonomy, interpersonal connection, and empathy

Automated AI systems which can make decisions for us can have potentially dehumanising consequences for people subject to them. It is one thing if an automated system is deciding whether an email should count as spam or not, and quite another if an AI is in charge of allocating scarse social services, or deciding who gets hired for a job. 

Individuals may feel disempowered in the face of unstoppable automation, especially when these decisions are relevant to their sense of personal autonomy. 

People may also feel like they are being merely "reduced to a statistic" by these systems, or that the use of their personal data violated their privacy.

Automation may also result in loss of empathy and crucial human connection. 

!!! example "1"
    Example 1

## 2. Poor qualities and dangerous outcomes

Inaccuracies, measurement errors, and sampling biases across data collection and recording can taint datasets. Using poor quality data could have grave consequences for individual wellbeing, and public welfare.

!!! example "2"
    Example 2

## 3. Bias, injustice, and discrimination

Supervised machine learning models draw insights (learn) from the existing data patterns on which they are trained. When they are working reliably, thet make accurate, out-of-sample predictions from what they inferred from the training data - regardless of whether these patterns are inequitable, biased, or discriminatory. 

This means that as long as supervised machine learning models are trained on historical data, in which past and present injustices, discrimination, and biases are deeply embedded, the algorithms will most likely reproduce, and even amplify, said injustices.

!!! example "3"
    - Healthcare models
    - Recidivism predictors
    - Hiring algorithms


## 4. Widening global and digital divides

!!! example "4"
    Example 4

## 5. Data integrity, privacy, and security

!!! example "5"
    Example 5

## 6. Biospheric harms

The explosion of computing power (which has partly driven the “big data revolution”) has had significant environmental cost. The development of computationally intensive algorithmic models which require large amounts of data requires high levels of energy consumption. 

Many of these models ingest abundant amounts of data for training, tuning, iterative model selection. These increase in model size and complexity does not necessarily lead to an equally large increase in model accuracy; in many cases only the gains in accuracy are only modest. The amount of compute needed to train complex models has increased 300,000 times from 2013 to 2019, with training expenditures of energy doubling every six months. These means that a lot of costly resources are used, even when the benefits of improvements in the model are small at best. Additionally, the costs of these resources are burdened upon everyone on the planet (in the form of negative externalities), while the modest gain in model performace is most likely accrued to the propietary owner(s) of the model.

These models contribute to emissions which are partly responsible of biospheric harm and climate change. Additionally, the distribution of the benefits and risks of the use of data-intensive models is not uniformly distributed among the population or among the world’s regions.
If anything, the allocations of benefit and risk have closely tracked the existing patterns of environmental racism, coloniality, and “slow violence” (Nixon, 2011 (CITE)) that have typified the disproportionate exposure of marginalised communities (especially those who inhabit what has conventionally been referred to as “the Global South”) to the pollution and destruction of local ecosystems and to involuntary displacement.

!!! example "6"
    - Google’s large language model BERT. Training it produces emissions equivalent to around 1 transatlantic flight (Strubell et al., 2019). 